"""
æ±‡æŠ¥åŠ©æ‰‹åŠŸèƒ½æ¨¡å—
å°†ç¢ç‰‡åŒ–çš„å·¥ä½œä¿¡æ¯è½¬åŒ–ä¸ºç»“æ„åŒ–çš„æ±‡æŠ¥æ–‡æ¡ˆ
"""

import streamlit as st

from ..core.api import call_gemini_stream
from ..core.chat import (
    init_chat_history, add_chat_message, get_chat_history,
    clear_chat_history, build_chat_context
)
from ..core.history import add_to_history
from ..config.prompts import REPORT_ASSISTANT_SYSTEM_PROMPT


def render_report_assistant():
    """æ¸²æŸ“æ±‡æŠ¥åŠ©æ‰‹åŠŸèƒ½ç•Œé¢"""
    st.markdown("### ğŸ“Š æ±‡æŠ¥åŠ©æ‰‹")
    st.markdown("å°†ç¢ç‰‡åŒ–çš„å·¥ä½œä¿¡æ¯è½¬åŒ–ä¸ºç»“æ„åŒ–çš„æ±‡æŠ¥æ–‡æ¡ˆï¼Œç”¨äºå‘é¢†å¯¼åŒæ­¥å·¥ä½œäº‹é¡¹ã€‚")
    
    # ä¸‰ä¸ªç‹¬ç«‹çš„è¾“å…¥æ¡†
    col1, col2 = st.columns([1, 1])
    
    with col1:
        current_problem = st.text_area(
            "ğŸ“Œ å½“å‰é—®é¢˜ (Current Problem)",
            height=150,
            placeholder="æè¿°å½“å‰é‡åˆ°çš„é—®é¢˜æˆ–èƒŒæ™¯...\n\nä¾‹å¦‚ï¼š\nå½“å‰ç”¨æˆ·åé¦ˆæ¸¸æˆå†…å¥½å‹æ·»åŠ æµç¨‹ç¹çï¼Œéœ€è¦æ‰‹åŠ¨è¾“å…¥IDï¼Œä¸”æ²¡æœ‰æ¨èå¥½å‹åŠŸèƒ½...",
            key="report_problem"
        )
        
        expected_result = st.text_area(
            "ğŸ¯ é¢„æœŸç»“æœ (Expected Result)",
            height=150,
            placeholder="æè¿°æœŸæœ›è¾¾æˆçš„æ•ˆæœ...\n\nä¾‹å¦‚ï¼š\nå¥½å‹æ·»åŠ æˆåŠŸç‡æå‡30%ï¼Œç”¨æˆ·å¥½å‹æ•°é‡å¹³å‡å¢åŠ 2ä¸ª...",
            key="report_result"
        )
    
    with col2:
        solution = st.text_area(
            "ğŸ’¡ è§£å†³æ–¹æ¡ˆ (Solution)",
            height=332,
            placeholder="æè¿°æ‚¨çš„è§£å†³æ–¹æ¡ˆæˆ–è®¡åˆ’é‡‡å–çš„æªæ–½...\n\nä¾‹å¦‚ï¼š\n1. æ–°å¢ã€Œå¯èƒ½è®¤è¯†çš„äººã€æ¨èåˆ—è¡¨\n2. æ”¯æŒé€šè¿‡æ¸¸æˆå†…æ˜µç§°æœç´¢\n3. æ·»åŠ å¥½å‹åè‡ªåŠ¨å‘é€ä¸€æ¡æ‹›å‘¼è¯­...",
            key="report_solution"
        )
    
    # åˆå§‹åŒ–æ±‡æŠ¥åŠ©æ‰‹ç›¸å…³çš„session_state
    if "generated_report" not in st.session_state:
        st.session_state.generated_report = ""
    if "report_processing" not in st.session_state:
        st.session_state.report_processing = False
    
    # ç”ŸæˆæŒ‰é’®
    if st.button("ğŸ“ ç”Ÿæˆæ±‡æŠ¥", type="primary", disabled=st.session_state.report_processing):
        # éªŒè¯è¾“å…¥
        if not current_problem.strip():
            st.error("è¯·å¡«å†™ã€å½“å‰é—®é¢˜ã€‘ï¼")
        elif not solution.strip():
            st.error("è¯·å¡«å†™ã€è§£å†³æ–¹æ¡ˆã€‘ï¼")
        elif not expected_result.strip():
            st.error("è¯·å¡«å†™ã€é¢„æœŸç»“æœã€‘ï¼")
        else:
            st.session_state.report_processing = True
            st.session_state.should_stop = False
            st.session_state.generated_report = ""
            st.session_state.report_saved_to_history = False
            st.rerun()
    
    # å¤„ç†ç”Ÿæˆé˜¶æ®µ
    if st.session_state.report_processing:
        _process_report_generation(current_problem, solution, expected_result)
    
    # æ˜¾ç¤ºå·²ç”Ÿæˆçš„æ±‡æŠ¥ï¼ˆéå¤„ç†ä¸­çŠ¶æ€ï¼‰
    if st.session_state.generated_report and not st.session_state.report_processing:
        _display_report_result()


def _process_report_generation(current_problem: str, solution: str, expected_result: str):
    """å¤„ç†æ±‡æŠ¥ç”Ÿæˆè¿‡ç¨‹"""
    # æ˜¾ç¤ºä¸­æ­¢æŒ‰é’®å’ŒçŠ¶æ€
    col_status, col_stop = st.columns([4, 1])
    with col_status:
        st.markdown("**âœï¸ æ­£åœ¨ç”Ÿæˆæ±‡æŠ¥æ–‡æ¡ˆ...**")
    with col_stop:
        if st.button("â¹ï¸ ä¸­æ­¢ç”Ÿæˆ", key="stop_report", type="secondary"):
            st.session_state.should_stop = True
            st.warning("æ­£åœ¨ä¸­æ­¢...")
    
    # æ€è€ƒè¿‡ç¨‹å±•ç¤ºåŒºåŸŸ
    thinking_expander = st.expander("ğŸ’­ æŸ¥çœ‹æ¨¡å‹æ€è€ƒè¿‡ç¨‹", expanded=False)
    with thinking_expander:
        thinking_container = st.empty()
    
    # è¾“å‡ºå®¹å™¨
    output_container = st.empty()
    
    # æ„å»ºPrompt
    user_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼Œæ’°å†™ä¸€ä»½ç»™é¢†å¯¼çš„å·¥ä½œæ±‡æŠ¥æ–‡æ¡ˆï¼š

ã€å½“å‰é—®é¢˜ã€‘
{current_problem}

ã€è§£å†³æ–¹æ¡ˆã€‘
{solution}

ã€é¢„æœŸç»“æœã€‘
{expected_result}

è¯·æŒ‰ç…§æ¨¡æ¿æ ¼å¼è¾“å‡ºæ±‡æŠ¥æ–‡æ¡ˆã€‚"""
    
    # è°ƒç”¨Gemini APIï¼ˆæµå¼ï¼‰
    full_response = ""
    thinking_content = ""
    was_stopped = False
    has_error = False
    error_message = ""
    
    for chunk in call_gemini_stream(user_prompt, REPORT_ASSISTANT_SYSTEM_PROMPT, thinking_container):
        if st.session_state.should_stop:
            was_stopped = True
            break
        
        if chunk["type"] == "text":
            full_response += chunk["content"]
            output_container.markdown(full_response + "â–Œ")
        elif chunk["type"] == "thinking":
            thinking_content += chunk["content"]
            with thinking_expander:
                thinking_container.markdown(thinking_content)
        elif chunk["type"] == "error":
            has_error = True
            error_message = chunk["content"]
            break
        elif chunk["type"] == "retry":
            st.info(chunk["content"])
    
    # ç§»é™¤å…‰æ ‡
    if full_response:
        output_container.markdown(full_response)
    
    # å¤„ç†ç»“æœ
    if has_error:
        st.error(f"âŒ ç”Ÿæˆå¤±è´¥: {error_message}")
    elif was_stopped:
        st.warning("âš ï¸ ç”Ÿæˆå·²ä¸­æ­¢")
        if full_response:
            st.session_state.generated_report = full_response
    else:
        st.success("âœ… æ±‡æŠ¥æ–‡æ¡ˆç”Ÿæˆå®Œæˆï¼")
        st.session_state.generated_report = full_response
    
    st.session_state.report_processing = False
    st.session_state.should_stop = False
    st.rerun()


def _display_report_result():
    """æ˜¾ç¤ºæ±‡æŠ¥ç»“æœå’Œå¤šè½®å¯¹è¯ç•Œé¢"""
    st.markdown("### ğŸ“„ ç”Ÿæˆçš„æ±‡æŠ¥æ–‡æ¡ˆ")
    st.markdown(st.session_state.generated_report)
    
    # å¤åˆ¶æŒ‰é’®ï¼ˆä½¿ç”¨ä¸‹è½½æŒ‰é’®æ¨¡æ‹Ÿï¼‰
    st.download_button(
        label="ğŸ“‹ ä¸‹è½½æ±‡æŠ¥æ–‡æ¡ˆ (TXT)",
        data=st.session_state.generated_report,
        file_name="å·¥ä½œæ±‡æŠ¥.txt",
        mime="text/plain"
    )
    
    # ä¿å­˜åˆ°ä¼šè¯å†å²ï¼ˆä»…åœ¨é¦–æ¬¡å®Œæˆæ—¶ä¿å­˜ï¼Œé¿å…é‡å¤ï¼‰
    if not st.session_state.get("report_saved_to_history"):
        add_to_history(
            function_type="æ±‡æŠ¥åŠ©æ‰‹",
            input_data={
                "å½“å‰é—®é¢˜": st.session_state.get("report_problem", ""),
                "è§£å†³æ–¹æ¡ˆ": st.session_state.get("report_solution", ""),
                "é¢„æœŸç»“æœ": st.session_state.get("report_result", "")
            },
            output_data=st.session_state.generated_report,
            download_data=st.session_state.generated_report.encode("utf-8"),
            download_filename="å·¥ä½œæ±‡æŠ¥.txt",
            download_mime="text/plain"
        )
        st.session_state.report_saved_to_history = True
    
    # ========== å¤šè½®å¯¹è¯åŒºåŸŸ ==========
    st.markdown("---")
    st.markdown("### ğŸ’¬ ç»§ç»­å¯¹è¯")
    st.caption("æ‚¨å¯ä»¥ç»§ç»­è¿½é—®æˆ–è¦æ±‚ä¿®æ”¹ï¼ŒAIå°†åŸºäºå·²ç”Ÿæˆçš„æ±‡æŠ¥æ–‡æ¡ˆè¿›è¡Œå›ç­”ã€‚")
    
    # åˆå§‹åŒ–å¯¹è¯å†å²
    chat_key = "report_chat"
    init_chat_history(chat_key)
    
    # æ˜¾ç¤ºå¯¹è¯å†å²
    chat_history = get_chat_history(chat_key)
    if chat_history:
        for msg in chat_history:
            if msg["role"] == "user":
                st.markdown(f"**ğŸ§‘ ç”¨æˆ·** _{msg['timestamp']}_")
                st.info(msg["content"])
            else:
                st.markdown(f"**ğŸ¤– åŠ©æ‰‹** _{msg['timestamp']}_")
                st.markdown(msg["content"])
    
    # å¯¹è¯è¾“å…¥
    report_chat_col1, report_chat_col2, report_chat_col3 = st.columns([6, 1, 1])
    with report_chat_col1:
        report_chat_input = st.text_input(
            "è¿½é—®æˆ–ä¿®æ”¹è¦æ±‚",
            placeholder="ä¾‹å¦‚ï¼šè¯·æŠŠè§£å†³æ–¹æ¡ˆå†™å¾—æ›´è¯¦ç»†ä¸€äº›...",
            key="report_chat_input",
            label_visibility="collapsed"
        )
    with report_chat_col2:
        report_chat_send = st.button("å‘é€", key="report_chat_send", type="primary", use_container_width=True)
    with report_chat_col3:
        if st.button("æ¸…ç©º", key="report_chat_clear", use_container_width=True):
            clear_chat_history(chat_key)
            st.rerun()
    
    # å¤„ç†å¯¹è¯
    if report_chat_send and report_chat_input.strip():
        add_chat_message(chat_key, "user", report_chat_input)
        
        # æ„å»ºä¸Šä¸‹æ–‡
        function_context = f"""ã€å·²ç”Ÿæˆçš„æ±‡æŠ¥æ–‡æ¡ˆã€‘
{st.session_state.generated_report}"""
        
        history_context = build_chat_context(chat_key, REPORT_ASSISTANT_SYSTEM_PROMPT)
        full_prompt = f"""{function_context}

{history_context}

ã€å½“å‰ç”¨æˆ·è¾“å…¥ã€‘
{report_chat_input}

è¯·åŸºäºä»¥ä¸Šæ±‡æŠ¥æ–‡æ¡ˆå’Œå¯¹è¯å†å²ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜æˆ–æŒ‰è¦æ±‚è¿›è¡Œä¿®æ”¹ã€‚å¦‚æœç”¨æˆ·è¦æ±‚ä¿®æ”¹ï¼Œè¯·è¾“å‡ºä¿®æ”¹åçš„å®Œæ•´å†…å®¹ã€‚"""
        
        with st.spinner("æ­£åœ¨æ€è€ƒ..."):
            response_container = st.empty()
            full_response = ""
            for chunk in call_gemini_stream(full_prompt, REPORT_ASSISTANT_SYSTEM_PROMPT):
                if chunk["type"] == "text":
                    full_response += chunk["content"]
                    response_container.markdown(full_response + "â–Œ")
                elif chunk["type"] == "error":
                    st.error(f"ç”Ÿæˆå¤±è´¥: {chunk['content']}")
                    break
            
            if full_response:
                response_container.markdown(full_response)
                add_chat_message(chat_key, "assistant", full_response)
                st.rerun()
