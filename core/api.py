"""
Gemini API è°ƒç”¨æ¨¡å—
å°è£…ä¸Google Gemini APIçš„æ‰€æœ‰äº¤äº’
"""

import streamlit as st
from google import genai
from google.genai import types
from typing import Optional, Generator
import time

from ..config.models import AVAILABLE_MODELS, FILE_UPLOAD_SUPPORTED_MODELS


def get_gemini_client():
    """è·å–Geminiå®¢æˆ·ç«¯å®ä¾‹"""
    api_key = st.session_state.get("api_key", "")
    if not api_key:
        st.error("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½® API Key")
        return None
    try:
        client = genai.Client(api_key=api_key)
        return client
    except Exception as e:
        st.error(f"APIåˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return None


def get_selected_model():
    """è·å–å½“å‰é€‰æ‹©çš„æ¨¡å‹"""
    return st.session_state.get("selected_model", AVAILABLE_MODELS[0])


def fetch_available_models():
    """ä»APIè·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    api_key = st.session_state.get("api_key", "")
    if not api_key:
        return []
    try:
        client = genai.Client(api_key=api_key)
        models = []
        for model in client.models.list():
            # åªè·å–æ”¯æŒgenerateContentçš„æ¨¡å‹
            if hasattr(model, 'supported_actions') and 'generateContent' in model.supported_actions:
                models.append(model.name.replace("models/", ""))
            elif not hasattr(model, 'supported_actions'):
                # å¦‚æœæ²¡æœ‰supported_actionså±æ€§ï¼Œä¹Ÿæ·»åŠ ï¼ˆå…¼å®¹æ€§å¤„ç†ï¼‰
                model_name = model.name.replace("models/", "")
                if 'gemini' in model_name.lower():
                    models.append(model_name)
        return sorted(models) if models else AVAILABLE_MODELS
    except Exception as e:
        st.sidebar.warning(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ—è¡¨: {str(e)}")
        return AVAILABLE_MODELS


def is_file_upload_supported() -> bool:
    """æ£€æŸ¥å½“å‰é€‰æ‹©çš„æ¨¡å‹æ˜¯å¦æ”¯æŒæ–‡ä»¶ä¸Šä¼ """
    current_model = get_selected_model()
    # æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦åœ¨æ”¯æŒåˆ—è¡¨ä¸­ï¼ˆéƒ¨åˆ†åŒ¹é…ï¼‰
    for supported_model in FILE_UPLOAD_SUPPORTED_MODELS:
        if supported_model in current_model or current_model in supported_model:
            return True
    return False


def call_gemini(prompt: str, system_prompt: str = "") -> Optional[str]:
    """
    è°ƒç”¨Gemini APIï¼ˆéæµå¼ï¼Œç”¨äºå†…éƒ¨å¤„ç†ï¼‰
    
    Args:
        prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
        system_prompt: ç³»ç»Ÿæç¤ºè¯
    
    Returns:
        APIè¿”å›çš„æ–‡æœ¬å†…å®¹ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        client = get_gemini_client()
        if client is None:
            return None
        
        # æ„å»ºé…ç½®
        config = types.GenerateContentConfig(
            system_instruction=system_prompt if system_prompt else None
        )
        
        response = client.models.generate_content(
            model=get_selected_model(),
            contents=prompt,
            config=config
        )
        return response.text
    except Exception as e:
        st.error(f"APIè°ƒç”¨å¤±è´¥: {str(e)}")
        return None


def call_gemini_stream(prompt: str, system_prompt: str = "", thinking_container=None) -> Generator[dict, None, None]:
    """
    æµå¼è°ƒç”¨Gemini APIï¼Œæ”¯æŒä¸­æ­¢ã€é”™è¯¯å±•ç¤ºã€æ€è€ƒè¿‡ç¨‹å’Œè‡ªåŠ¨é‡è¯•
    
    Args:
        prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        thinking_container: ç”¨äºæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹çš„å®¹å™¨ï¼ˆå¯é€‰ï¼‰
    
    Yields:
        dict: {"type": "text"|"thinking"|"error"|"retry", "content": str}
    """
    # æ¸…ç©ºä¹‹å‰çš„é”™è¯¯
    st.session_state.last_error = ""
    st.session_state.thinking_content = ""
    
    # é‡è¯•é…ç½®
    max_retries = 3
    retry_delay = 5  # ç§’
    retryable_errors = ["503", "429", "overloaded", "UNAVAILABLE", "RESOURCE_EXHAUSTED", "rate limit"]
    
    for attempt in range(max_retries):
        try:
            client = get_gemini_client()
            if client is None:
                yield {"type": "error", "content": "APIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥API Key"}
                return
            
            # æ„å»ºé…ç½® - å¯ç”¨æ€è€ƒè¿‡ç¨‹ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
            config = types.GenerateContentConfig(
                system_instruction=system_prompt if system_prompt else None,
                # å°è¯•å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆéƒ¨åˆ†æ¨¡å‹æ”¯æŒï¼‰
                thinking_config=types.ThinkingConfig(
                    thinking_budget=10000  # å…è®¸çš„æ€è€ƒtokenæ•°
                ) if "2.5" in get_selected_model() or "think" in get_selected_model().lower() else None
            )
            
            # ä½¿ç”¨æµå¼API
            response_stream = client.models.generate_content_stream(
                model=get_selected_model(),
                contents=prompt,
                config=config
            )
            
            for chunk in response_stream:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸­æ­¢
                if st.session_state.should_stop:
                    yield {"type": "stopped", "content": "ç”¨æˆ·å·²ä¸­æ­¢ç”Ÿæˆ"}
                    st.session_state.should_stop = False
                    return
                
                # å¤„ç†æ€è€ƒè¿‡ç¨‹ï¼ˆå¦‚æœæœ‰ï¼‰
                if hasattr(chunk, 'candidates') and chunk.candidates:
                    for candidate in chunk.candidates:
                        if hasattr(candidate, 'content') and candidate.content:
                            for part in candidate.content.parts:
                                # æ£€æŸ¥æ˜¯å¦æ˜¯æ€è€ƒå†…å®¹
                                if hasattr(part, 'thought') and part.thought:
                                    thinking_text = part.text if hasattr(part, 'text') else str(part)
                                    st.session_state.thinking_content += thinking_text
                                    yield {"type": "thinking", "content": thinking_text}
                                elif hasattr(part, 'text') and part.text:
                                    yield {"type": "text", "content": part.text}
                elif chunk.text:
                    yield {"type": "text", "content": chunk.text}
            
            # æˆåŠŸå®Œæˆï¼Œé€€å‡ºé‡è¯•å¾ªç¯
            return
                    
        except Exception as e:
            error_msg = str(e)
            st.session_state.last_error = error_msg
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¯é‡è¯•çš„é”™è¯¯
            is_retryable = any(err_key in error_msg for err_key in retryable_errors)
            
            if is_retryable and attempt < max_retries - 1:
                # é€šçŸ¥ç”¨æˆ·æ­£åœ¨é‡è¯•
                remaining = max_retries - attempt - 1
                yield {
                    "type": "retry", 
                    "content": f"âš ï¸ æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ ({error_msg[:50]}...)ï¼Œ{retry_delay}ç§’åè‡ªåŠ¨é‡è¯•ï¼ˆå‰©ä½™{remaining}æ¬¡ï¼‰..."
                }
                time.sleep(retry_delay)
                # å¢åŠ ä¸‹æ¬¡é‡è¯•çš„ç­‰å¾…æ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
                retry_delay = min(retry_delay * 2, 30)
                continue
            else:
                # ä¸å¯é‡è¯•æˆ–å·²ç”¨å®Œé‡è¯•æ¬¡æ•°
                yield {"type": "error", "content": error_msg}
                return


def stream_to_container(prompt: str, system_prompt: str, container, thinking_container=None, status_container=None) -> tuple:
    """
    æµå¼è¾“å‡ºåˆ°Streamlitå®¹å™¨ï¼Œå®æ—¶æ˜¾ç¤ºæ‰“å­—æ•ˆæœï¼Œæ”¯æŒä¸­æ­¢ã€é”™è¯¯å±•ç¤ºå’Œæ€è€ƒè¿‡ç¨‹
    
    Args:
        prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        container: Streamlitå®¹å™¨å¯¹è±¡ï¼ˆå¦‚st.empty()æˆ–st.container()ï¼‰
        thinking_container: ç”¨äºæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹çš„å®¹å™¨ï¼ˆå¯é€‰ï¼‰
        status_container: ç”¨äºæ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯çš„å®¹å™¨ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        tuple: (å®Œæ•´çš„å“åº”æ–‡æœ¬, æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯)
    """
    full_response = ""
    thinking_text = ""
    error_msg = ""
    was_stopped = False
    
    # ä½¿ç”¨ç”Ÿæˆå™¨è¿›è¡Œæµå¼è¾“å‡º
    for chunk_data in call_gemini_stream(prompt, system_prompt, thinking_container):
        chunk_type = chunk_data.get("type", "text")
        chunk_content = chunk_data.get("content", "")
        
        if chunk_type == "text":
            full_response += chunk_content
            # å®æ—¶æ›´æ–°æ˜¾ç¤ºå†…å®¹ï¼Œæ·»åŠ å…‰æ ‡æ•ˆæœ
            container.markdown(full_response + " â–Œ")
        elif chunk_type == "thinking":
            thinking_text += chunk_content
            # æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
            if thinking_container:
                thinking_container.markdown(f"ğŸ’­ **æ¨¡å‹æ€è€ƒä¸­...**\n\n{thinking_text}")
        elif chunk_type == "retry":
            # æ˜¾ç¤ºé‡è¯•çŠ¶æ€
            if status_container:
                status_container.warning(chunk_content)
            else:
                st.warning(chunk_content)
        elif chunk_type == "error":
            error_msg = chunk_content
            if status_container:
                status_container.error(f"âŒ APIè°ƒç”¨å¤±è´¥: {error_msg}")
            else:
                st.error(f"âŒ APIè°ƒç”¨å¤±è´¥: {error_msg}")
            break
        elif chunk_type == "stopped":
            was_stopped = True
            if status_container:
                status_container.warning("â¹ï¸ ç”Ÿæˆå·²ä¸­æ­¢")
            break
        
        # å¼ºåˆ¶åˆ·æ–°æ˜¾ç¤º
        time.sleep(0.01)
    
    # ç§»é™¤å…‰æ ‡ï¼Œæ˜¾ç¤ºæœ€ç»ˆç»“æœ
    if full_response:
        container.markdown(full_response)
    
    # åˆ¤æ–­æ˜¯å¦æˆåŠŸ
    success = bool(full_response) and not error_msg and not was_stopped
    
    return (full_response, success, error_msg)


def stream_generator(prompt: str, system_prompt: str):
    """
    åˆ›å»ºæµå¼è¾“å‡ºç”Ÿæˆå™¨ï¼Œé…åˆst.write_streamä½¿ç”¨
    
    Args:
        prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
        system_prompt: ç³»ç»Ÿæç¤ºè¯
    
    Yields:
        æ–‡æœ¬ç‰‡æ®µ
    """
    for chunk_data in call_gemini_stream(prompt, system_prompt):
        if chunk_data.get("type") == "text":
            yield chunk_data.get("content", "")
